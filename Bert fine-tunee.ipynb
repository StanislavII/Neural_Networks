{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7191bd4",
   "metadata": {},
   "source": [
    "# Классификация договоров по риску - нет риска, есть риск, аренда (втф)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc84e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import re, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa01219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from babel.dates import format_date, format_datetime, format_time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a055cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1152eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc968b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymystem3\n",
    "import gensim\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9efa0815",
   "metadata": {},
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model_bert = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82dccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sber = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "model_sber = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19931e19",
   "metadata": {},
   "source": [
    "* здесь используются выжимки из python-docx абзац наиболее важной части контракта (не шаблонной)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9d8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'model_out_total.xlsx')\n",
    "data = data[['Шаблон', 'Флаг']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c01c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1816ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stanislavilusin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b685b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stanislavilusin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f59a2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b99b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords.extend(['один','два',\"три\",\"четыре\",\"пять\",\"шесть\",\"семь\",\"восемь\",\"девять\",\"сто\",\"двести\",\n",
    "                          \"триста\",\"четыреста\",\"пятьсот\",\"шестьсот\",\"восемьсот\",\"девятьсот\",\"миллион\",\"рубль\",\"копейка\",\n",
    "                         'семьсот',\"десять\",\"двадцать\",\"тридцать\",\"сорок\",\"пятьдесят\",\"шестьдесят\",\"семьдесят\",\"восемьдесят\",\n",
    "                          \"девяносто\",\"тысяча\",\"одиннадцать\",\"двенадцать\",\"тринадцать\",\"четырнадцать\",\"пятнадцать\",\n",
    "                          \"шестнадцать\",\"семнадцать\",\"восемнадцать\", \"девятнадцать\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b2be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=|_|__|___)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower())+ ' '.join(emoticons).replace('-', '')).replace('_', '')\n",
    "    \n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language = 'russian'):\n",
    "        for word in nltk.word_tokenize(sent, language = 'russian'):\n",
    "            if len(word) <= 3:\n",
    "                continue\n",
    "            word = morph.parse(word.lower())[0].normal_form\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec6f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    data['Шаблон'].loc[index] = ' '.join(preprocess_text(row['Шаблон']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "524d5983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "постоплата    252\n",
       "предоплата    244\n",
       "помесячно     141\n",
       "Name: Флаг, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Флаг'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c12183",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_risk = np.array(data['Флаг'])\n",
    "y_risk = np.where(y_risk == 'постоплата' ,int(0), y_risk)\n",
    "y_risk = np.where(y_risk == 'предоплата' ,int(1), y_risk)\n",
    "y_risk = np.where(y_risk == 'помесячно' ,int(2), y_risk)\n",
    "y_risk = y_risk.astype('int')\n",
    "data['Флаг'] = y_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44c5d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split(data['Шаблон'], data['Флаг'], \n",
    "                                                                random_state=2022, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=data['Флаг'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0b31c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Привет меня зовут Стас\", \"Это модель сбербанка\", \"Здесь я выведу хрень хрень что-то\"]\n",
    "sent_id = tokenizer_sber.batch_encode_plus(text, padding=True, return_token_type_ids = True, max_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3c431b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 6571, 1024, 10160, 791, 381, 102, 0, 0, 0, 0], [101, 736, 7537, 83492, 9408, 102, 0, 0, 0, 0, 0], [101, 1640, 119, 14077, 724, 46779, 46779, 693, 133, 696, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d23ff3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512 #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58aff2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_special_tokens=True \n",
    "# padding=\"longest\"\n",
    "# return_attention_mask=True\n",
    "# pad_to_max_length=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ae21dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_val = tokenizer_sber.batch_encode_plus(val_text.tolist(), max_length = max_seq_len, padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False)\n",
    "tokens_test = tokenizer_sber.batch_encode_plus(test_text.tolist(), max_length = max_seq_len,padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b19bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a8f9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 #?\n",
    "\n",
    "\n",
    "# tensor\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler loader\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7501e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "\n",
    "# sampler loader\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ab78b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_sber.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82a79579",
   "metadata": {},
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.relu =  nn.SELU()\n",
    "        self.fc1 = nn.Linear(1024,128) #1024,256\n",
    "        self.fc2 = nn.Linear(128,3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)  \n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a21007e",
   "metadata": {},
   "source": [
    "def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size= (3, 768), padding=True) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=768, stride=1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(9118464, 3)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)\n",
    "        # all_layers  = [32, 13, 64, 768]\n",
    "        x = torch.cat(all_layers, 0) # x= [416, 64, 768]\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6c1894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        self.conv = nn.Conv2d(in_channels=25, out_channels= 25, kernel_size= (3,1024),stride = (4,1), padding = (2,1)) #, stride = (5,2), padding = (3,3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1, padding = 1)#, padding = (1,1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(.05)\n",
    "        self.relu =  nn.SELU()\n",
    "        self.fc1 = nn.Linear(2475,256)\n",
    "        self.fc2 = nn.Linear(256,3)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True, return_dict=False) #return_dict=False)\n",
    "        \n",
    "        global test_bert\n",
    "        \n",
    "        test_bert =  self.bert(sent_id, attention_mask=mask, output_hidden_states=True, return_dict=False)\n",
    "        \n",
    "        a1,a2,a3 = self.bert(sent_id, attention_mask=mask, output_hidden_states=True, return_dict=False)\n",
    "        print(a1.shape,a2.shape) # (8,8,1024) (8,1024)\n",
    "        x = torch.cat(all_layers, 0)\n",
    "        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n",
    "        del all_layers\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        #x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n",
    "        #x = self.fc(self.dropout(self.flat(self.dropout(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04c6129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b358848",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(model_sber)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86f302d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "79a10f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84411277 0.87008547 1.50147493]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', classes = np.unique(val_labels), y = val_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a9d22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3b587cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "       if step % 10 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "       batch = [r.to(device) for r in batch]\n",
    " \n",
    "       sent_id, mask, labels = batch\n",
    "        \n",
    "    \n",
    "    # clear previously calculated gradients \n",
    "       model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "       \n",
    "        \n",
    "       preds = model(sent_id, mask)\n",
    "\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "       loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "       total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "       loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "       torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "       optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "       preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "       total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "def evaluate():\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "    for step,batch in enumerate(test_dataloader):\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0, format = 'medium', locale='rus')\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\n",
    "            print(elapsed)\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    \n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds,labels)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(test_dataloader) \n",
    "    avg_loss = np.log(avg_loss)\n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    train_loss, _ = train()\n",
    "    valid_loss, _= evaluate()\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print('YES!')\n",
    "        torch.save(model.state_dict(), 'saved_weights_sber_16_100_new_cnn_linear.pt')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "73f7b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'saved_weights_sber_16_100_new_cnn_linear.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af599e",
   "metadata": {},
   "source": [
    "### Final - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "313095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.71        51\n",
      "           1       0.78      0.71      0.74        49\n",
      "           2       0.89      0.57      0.70        28\n",
      "\n",
      "    accuracy                           0.72       128\n",
      "   macro avg       0.77      0.70      0.72       128\n",
      "weighted avg       0.74      0.72      0.72       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0e0a02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  8  2]\n",
      " [14 35  0]\n",
      " [10  2 16]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afe64e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds_val = model(val_seq.to(device), val_mask.to(device))\n",
    "    preds_val = preds_val.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c81517b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74       118\n",
      "           1       0.78      0.87      0.82       127\n",
      "           2       0.83      0.85      0.84        59\n",
      "\n",
      "    accuracy                           0.80       304\n",
      "   macro avg       0.80      0.80      0.80       304\n",
      "weighted avg       0.80      0.80      0.79       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "preds_val = np.argmax(preds_val, axis = 1)\n",
    "print(classification_report(val_y, preds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5da80237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82  28   8]\n",
      " [ 15 110   2]\n",
      " [  6   3  50]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(val_y, preds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47295e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### white board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8c2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \n",
    "    if type(h_w) is not tuple:\n",
    "        h_w = (h_w, h_w)\n",
    "    \n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    \n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    \n",
    "    if type(pad) is not tuple:\n",
    "        pad = (pad, pad)\n",
    "    \n",
    "    h = (h_w[0] + (2 * pad[0]) - (dilation * (kernel_size[0] - 1)) - 1)// stride[0] + 1\n",
    "    w = (h_w[1] + (2 * pad[1]) - (dilation * (kernel_size[1] - 1)) - 1)// stride[1] + 1\n",
    "    \n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6da8b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output_shape(h_w = (256,1), kernel_size = (8,1024), stride = (4,1), pad = (2,1), dilation = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4daade44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_p = nn.Conv2d(8, 16, kernel_size = (3,3))\n",
    "hh = torch.randn(5,8,34,34)\n",
    "conv_ = conv_p(hh)\n",
    "conv_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76eb3a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 56, 56])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn2 = nn.BatchNorm2d(num_features=128)\n",
    "bn2_ = bn2(conv_)\n",
    "bn2_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51d9f9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 7, 7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_p = nn.MaxPool2d(kernel_size = 2)\n",
    "hh = torch.randn(5, 32, 14 , 14)\n",
    "\n",
    "out_max = max_p(hh)\n",
    "\n",
    "flat_p = nn.Flatten()\n",
    "\n",
    "out_max.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "87eb43f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1024, 1, 1])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_p(hh).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70c9741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_t = nn.ConvTranspose2d(in_channels = 448 , out_channels = 512, kernel_size=4, stride=4, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e894b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = torch.rand(16,448,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c07b2c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 14, 14])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_t(ten).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e21d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
