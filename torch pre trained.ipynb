{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c578df",
   "metadata": {},
   "source": [
    "# Модели на предобученных сетях ru-bert sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062e367",
   "metadata": {},
   "source": [
    "### импорт пакетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbccafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from numpy import save, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e39b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score\n",
    "from sklearn import utils\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4764dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pymystem3\n",
    "import gensim\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f70d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315ba1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36f472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c4cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihot_batch = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0]])\n",
    "vnon = (multihot_batch == torch.tensor(1)).nonzero(as_tuple=False)\n",
    "v0 = vnon[:,0]\n",
    "v1 = vnon[:,1]\n",
    "split_ind = ((torch.roll(v0, -1, 0) - v0) == 1).nonzero(as_tuple=False)[:,0] + 1\n",
    "split_size = torch.cat([split_ind[0].view(1),(torch.roll(split_ind, -1, 0) - split_ind)[:-1]])\n",
    "final_size = torch.tensor([torch.numel(v1) - torch.sum(split_size)])\n",
    "split_size = torch.cat([split_size, final_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c771b",
   "metadata": {},
   "source": [
    "### добавление русских стемов и лем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ea1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3699fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stanislavilusin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d941c583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stanislavilusin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e63d8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a10dc0",
   "metadata": {},
   "source": [
    "### добавление числительных на сумм конракта для удаления из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c719c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords.extend(['один','два',\"три\",\"четыре\",\"пять\",\"шесть\",\"семь\",\"восемь\",\"девять\",\"сто\",\"двести\",\n",
    "                          \"триста\",\"четыреста\",\"пятьсот\",\"шестьсот\",\"восемьсот\",\"девятьсот\",\"миллион\",\"рубль\",\"копейка\",\n",
    "                         'семьсот',\"десять\",\"двадцать\",\"тридцать\",\"сорок\",\"пятьдесят\",\"шестьдесят\",\"семьдесят\",\"восемьдесят\",\n",
    "                          \"девяносто\",\"тысяча\",\"одиннадцать\",\"двенадцать\",\"тринадцать\",\"четырнадцать\",\"пятнадцать\",\n",
    "                          \"шестнадцать\",\"семнадцать\",\"восемнадцать\", \"девятнадцать\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb664e",
   "metadata": {},
   "source": [
    "### функция для обработки (лемматизация токенизация удаления эмотиконов токенизация из пакетов нлтк и возвращение начальной формы токена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebdf781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=|_|__|___)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower())+ ' '.join(emoticons).replace('-', '')).replace('_', '')\n",
    "    \n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language = 'russian'):\n",
    "        for word in nltk.word_tokenize(sent, language = 'russian'):\n",
    "            if len(word) <= 3:\n",
    "                continue\n",
    "            word = morph.parse(word.lower())[0].normal_form\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21cb34b",
   "metadata": {},
   "source": [
    "## Модель на ru-bert tiny (предобученная классификация на основе модельных эмбеддингов по токенам)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3259f",
   "metadata": {},
   "source": [
    "### функция возвращения токен-слой к текст-слой через маску последнего скрытого слоя берта"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1550555e",
   "metadata": {},
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d1c51",
   "metadata": {},
   "source": [
    "### скачиваем токенизатор и модель "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f81b5d1",
   "metadata": {},
   "source": [
    "tokenizer_sber = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model_sber = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acf9ab",
   "metadata": {},
   "source": [
    "### загрузка данных для трейна и теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72fd9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'model_out_total.xlsx')\n",
    "data = data[['Шаблон', 'Флаг']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b47857",
   "metadata": {},
   "source": [
    "### приведение данных к очищенному стандарту, также объединение токенов в исходные предложение по предположению обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    data['Шаблон'].loc[index] = ' '.join(preprocess_text(row['Шаблон']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36eaf584",
   "metadata": {},
   "source": [
    "X = vectors\n",
    "\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "lr = LogisticRegression(class_weight= 'balanced',solver='newton-cg',fit_intercept=True).fit(X, y_train)\n",
    "y_pred = lr.predict(X)\n",
    "cv_lr_f1.append(cohen_kappa_score(y_train, y_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_1 = confusion_matrix(y_train, y_pred)\n",
    "    \n",
    "sgd = linear_model.SGDClassifier(max_iter=10000,tol=1e-4,loss='log',class_weight='balanced').fit(X, y_train)\n",
    "y_pred = sgd.predict(X)\n",
    "cv_lrsgd_f1.append(cohen_kappa_score(y_train, y_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_2 = confusion_matrix(y_train, y_pred)\n",
    "    \n",
    "sgd_huber = linear_model.SGDClassifier(max_iter=1000,tol=1e-3,alpha=20,loss='modified_huber',class_weight='balanced').fit(X, y_train)\n",
    "y_pred = sgd_huber.predict(X)\n",
    "cv_svcsgd_f1.append(cohen_kappa_score(y_train, y_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_3 = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3811c",
   "metadata": {},
   "source": [
    "### скачиваем предобученный токенизатор и модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc63eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sber = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "model_sber = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0e1e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = pd.get_dummies(data, columns = ['Флаг'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45d84d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr, data_val = train_test_split(data_tr, test_size=0.2, random_state=2022, stratify  = data['Флаг'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc7a7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82db80cb",
   "metadata": {},
   "source": [
    "class TorchDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len,lazy: bool = False):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "        return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69ced6",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = TorchDataset(tokenizer_sber, data_tr, lazy=True)\n",
    "dev_dataset = TorchDataset(tokenizer_sber, data_val, lazy=True)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE)#, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE)#, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90deb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer: tokenizer_sber, dataframe: pd.DataFrame, lazy: bool = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.lazy = lazy\n",
    "        if not self.lazy:\n",
    "            self.X = []\n",
    "            self.Y = []\n",
    "            for i, (row) in tqdm(dataframe.iterrows()):\n",
    "                x, y = self.row_to_tensor(self.tokenizer, row)\n",
    "                self.X.append(x)\n",
    "                self.Y.append(y)\n",
    "        else:\n",
    "            self.df = dataframe        \n",
    "    \n",
    "    @staticmethod\n",
    "    def row_to_tensor(tokenizer: tokenizer_sber, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        tokens = tokenizer.encode(row[\"Шаблон\"], add_special_tokens=True)\n",
    "        if len(tokens) > 256:\n",
    "            tokens = tokens[:255] + [tokens[-1]]\n",
    "        x = torch.LongTensor(tokens)\n",
    "        y = torch.FloatTensor(row[['Флаг_постоплата', 'Флаг_предоплата',\n",
    "       'Флаг_помесячно']])\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.lazy:\n",
    "            return len(self.df)\n",
    "        else:\n",
    "            return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        if not self.lazy:\n",
    "            return self.X[index], self.Y[index]\n",
    "        else:\n",
    "            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])\n",
    "            \n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n",
    "        -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = ToxicDataset(tokenizer_sber, data_tr, lazy= True)\n",
    "dev_dataset = ToxicDataset(tokenizer_sber, data_val, lazy= True)\n",
    "collate_fn = partial(collate_fn, device=device)\n",
    "BATCH_SIZE = 16\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540e63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c64640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.50147493 0.84411277 0.87008547]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_wts = compute_class_weight(class_weight = 'balanced', classes = np.unique(data.loc[data_tr.index]['Флаг']), y = data.loc[data_tr.index]['Флаг'])\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58931b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "criterion = nn.NLLLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd46c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert: model_sber, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier1 = nn.Linear(bert.config.hidden_size, 256) #256\n",
    "        self.classifier2 = nn.Linear(256,num_classes) #256\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids, head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "        x = self.classifier1(cls_output)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier2(x)\n",
    "        cls_output = self.softmax(x)\n",
    "    \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            multihot_batch = labels\n",
    "            vnon = (multihot_batch == torch.tensor(1)).nonzero(as_tuple=False)\n",
    "            v0 = vnon[:,0]\n",
    "            v1 = vnon[:,1]\n",
    "            split_ind = ((torch.roll(v0, -1, 0) - v0) == 1).nonzero(as_tuple=False)[:,0] + 1\n",
    "            split_size = torch.cat([split_ind[0].view(1),(torch.roll(split_ind, -1, 0) - split_ind)[:-1]])\n",
    "            final_size = torch.tensor([torch.numel(v1) - torch.sum(split_size)])\n",
    "            split_size = torch.cat([split_size, final_size])\n",
    "            loss = criterion(cls_output, v1)\n",
    "        return loss, cls_output\n",
    "\n",
    "model = BertClassifier(model_sber, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b9dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b3b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    best_loss = 100\n",
    "    pred = []\n",
    "    true = []\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        pred += outputs.detach().numpy().tolist()\n",
    "        true += y.detach().numpy().tolist()\n",
    "        #pred = np.argmax(pred, axis = 1) \n",
    "        #true = np.argmax(true, axis = 1)\n",
    "        #print(pred, true)\n",
    "        #correct_predictions += torch.sum(pred == y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if total_loss / len(iterator) < best_loss:\n",
    "            best_loss = total_loss / len(iterator)\n",
    "            torch.save(model.state_dict(), 'saved_weights_tables_16.pt')\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    result_class = np.argmax(pred, axis = 1) \n",
    "    label_class = np.argmax(true, axis = 1)\n",
    "    print(confusion_matrix(label_class, result_class))\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "    #return correct_predictions.double() / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, estate = False):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "            #correct_predictions += torch.sum(pred == y)\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    result_class = np.argmax(pred, axis = 1) \n",
    "    label_class = np.argmax(true, axis = 1)\n",
    "    if estate == True:\n",
    "        print(classification_report(label_class, result_class))\n",
    "    print(confusion_matrix(label_class, result_class))\n",
    "    for i, name in enumerate(['Флаг_постоплата', 'Флаг_предоплата',\n",
    "       'Флаг_помесячно']):\n",
    "        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")\n",
    "    #return correct_predictions.double() / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c7b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "EPOCH_NUM = 100\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = 10 ** 3\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3893e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_32 ~ 15\n",
    "# sbert_16 ~ ??\n",
    "# rus_32 ~ 75\n",
    "# rus_16 ~ 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1ad40b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 0 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [27:00<00:00, 50.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[125  72   4]\n",
      " [140  51   4]\n",
      " [ 64  40   9]]\n",
      "Train loss 1.0860347896814346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:53<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0  0]\n",
      " [49  0  0]\n",
      " [28  0  0]]\n",
      "Флаг_постоплата roc_auc 0.5884899414311179\n",
      "Флаг_предоплата roc_auc 0.4900542495479204\n",
      "Флаг_помесячно roc_auc 0.7517857142857143\n",
      "Evaluate loss 1.0573433637619019\n",
      "================================================== EPOCH 1 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [28:03<00:00, 52.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[201   0   0]\n",
      " [195   0   0]\n",
      " [113   0   0]]\n",
      "Train loss 1.0338994562625885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:21<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0  0]\n",
      " [49  0  0]\n",
      " [28  0  0]]\n",
      "Флаг_постоплата roc_auc 0.6643748408454292\n",
      "Флаг_предоплата roc_auc 0.6383363471971067\n",
      "Флаг_помесячно roc_auc 0.7789285714285714\n",
      "Evaluate loss 1.0020856857299805\n",
      "================================================== EPOCH 2 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [36:00<00:00, 67.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[201   0   0]\n",
      " [193   2   0]\n",
      " [113   0   0]]\n",
      "Train loss 0.9862015675753355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:39<00:00, 12.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  0  0]\n",
      " [37 12  0]\n",
      " [27  1  0]]\n",
      "Флаг_постоплата roc_auc 0.7201426024955436\n",
      "Флаг_предоплата roc_auc 0.6889692585895117\n",
      "Флаг_помесячно roc_auc 0.7989285714285714\n",
      "Evaluate loss 0.9468282461166382\n",
      "================================================== EPOCH 3 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [38:04<00:00, 71.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[201   0   0]\n",
      " [155  40   0]\n",
      " [104   1   8]]\n",
      "Train loss 0.9243378229439259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:45<00:00, 13.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  1  0]\n",
      " [33 16  0]\n",
      " [16  2 10]]\n",
      "Флаг_постоплата roc_auc 0.7713267125031831\n",
      "Флаг_предоплата roc_auc 0.7545853784551796\n",
      "Флаг_помесячно roc_auc 0.8407142857142857\n",
      "Evaluate loss 0.8746344447135925\n",
      "================================================== EPOCH 4 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [36:31<00:00, 68.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197   4   0]\n",
      " [130  65   0]\n",
      " [ 67   2  44]]\n",
      "Train loss 0.8441103305667639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:56<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  4  0]\n",
      " [26 23  0]\n",
      " [14  3 11]]\n",
      "Флаг_постоплата roc_auc 0.782785841609371\n",
      "Флаг_предоплата roc_auc 0.7969516920692328\n",
      "Флаг_помесячно roc_auc 0.8267857142857143\n",
      "Evaluate loss 0.8191222548484802\n",
      "================================================== EPOCH 5 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [25:22<00:00, 47.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  11   1]\n",
      " [103  92   0]\n",
      " [ 61   4  48]]\n",
      "Train loss 0.7868823055177927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:14<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 16  0]\n",
      " [15 34  0]\n",
      " [13  4 11]]\n",
      "Флаг_постоплата roc_auc 0.7914438502673797\n",
      "Флаг_предоплата roc_auc 0.8217514854042882\n",
      "Флаг_помесячно roc_auc 0.8496428571428571\n",
      "Evaluate loss 0.830758810043335\n",
      "================================================== EPOCH 6 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [25:59<00:00, 48.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184  15   2]\n",
      " [ 65 130   0]\n",
      " [ 50   5  58]]\n",
      "Train loss 0.6945188883692026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:10<00:00,  8.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36 14  1]\n",
      " [14 35  0]\n",
      " [ 9  2 17]]\n",
      "Флаг_постоплата roc_auc 0.7868601986249045\n",
      "Флаг_предоплата roc_auc 0.8057349522087316\n",
      "Флаг_помесячно roc_auc 0.88\n",
      "Evaluate loss 0.7733621001243591\n",
      "================================================== EPOCH 7 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [28:51<00:00, 54.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[187  10   4]\n",
      " [ 44 147   4]\n",
      " [ 35   4  74]]\n",
      "Train loss 0.6063835425302386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:25<00:00, 10.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40 10  1]\n",
      " [15 34  0]\n",
      " [ 7  3 18]]\n",
      "Флаг_постоплата roc_auc 0.8151260504201681\n",
      "Флаг_предоплата roc_auc 0.8165848617928183\n",
      "Флаг_помесячно roc_auc 0.8864285714285715\n",
      "Evaluate loss 0.7115986943244934\n",
      "================================================== EPOCH 8 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [33:23<00:00, 62.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189   9   3]\n",
      " [ 37 155   3]\n",
      " [ 33   7  73]]\n",
      "Train loss 0.5311950286850333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:28<00:00, 11.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32 18  1]\n",
      " [ 8 41  0]\n",
      " [ 7  5 16]]\n",
      "Флаг_постоплата roc_auc 0.7919531448943213\n",
      "Флаг_предоплата roc_auc 0.8483595970033584\n",
      "Флаг_помесячно roc_auc 0.8785714285714284\n",
      "Evaluate loss 0.7787710428237915\n",
      "================================================== EPOCH 9 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [36:03<00:00, 67.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[179  17   5]\n",
      " [ 28 163   4]\n",
      " [ 30   5  78]]\n",
      "Train loss 0.5129277305677533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:41<00:00, 12.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 15  5]\n",
      " [10 38  1]\n",
      " [ 5  2 21]]\n",
      "Флаг_постоплата roc_auc 0.8270944741532977\n",
      "Флаг_предоплата roc_auc 0.8599845001291656\n",
      "Флаг_помесячно roc_auc 0.8989285714285715\n",
      "Evaluate loss 0.725001871585846\n",
      "================================================== EPOCH 10 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [35:17<00:00, 66.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[191   3   7]\n",
      " [ 16 175   4]\n",
      " [ 19   4  90]]\n",
      "Train loss 0.37778008449822664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:42<00:00, 12.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37 13  1]\n",
      " [ 5 44  0]\n",
      " [ 7  3 18]]\n",
      "Флаг_постоплата roc_auc 0.8548510313216195\n",
      "Флаг_предоплата roc_auc 0.8984758460346163\n",
      "Флаг_помесячно roc_auc 0.8835714285714285\n",
      "Evaluate loss 0.6722517609596252\n",
      "================================================== EPOCH 11 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [34:26<00:00, 64.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192   5   4]\n",
      " [ 16 178   1]\n",
      " [ 20   5  88]]\n",
      "Train loss 0.3324918122962117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:30<00:00, 11.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 22  5]\n",
      " [ 2 45  2]\n",
      " [ 3  5 20]]\n",
      "Флаг_постоплата roc_auc 0.8293862999745353\n",
      "Флаг_предоплата roc_auc 0.9142340480495995\n",
      "Флаг_помесячно roc_auc 0.9146428571428571\n",
      "Evaluate loss 0.9057865738868713\n",
      "================================================== EPOCH 12 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [32:19<00:00, 60.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[193   3   5]\n",
      " [ 11 183   1]\n",
      " [ 12   5  96]]\n",
      "Train loss 0.27128787408582866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:29<00:00, 11.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37 13  1]\n",
      " [ 5 44  0]\n",
      " [ 5  2 21]]\n",
      "Флаг_постоплата roc_auc 0.846193022663611\n",
      "Флаг_предоплата roc_auc 0.9147507104107466\n",
      "Флаг_помесячно roc_auc 0.907142857142857\n",
      "Evaluate loss 0.7455014586448669\n",
      "================================================== EPOCH 13 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [29:13<00:00, 54.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194   2   5]\n",
      " [  6 189   0]\n",
      " [ 12   2  99]]\n",
      "Train loss 0.20627151452936232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:21<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33 13  5]\n",
      " [ 3 45  1]\n",
      " [ 3  2 23]]\n",
      "Флаг_постоплата roc_auc 0.8739495798319328\n",
      "Флаг_предоплата roc_auc 0.9173340222164815\n",
      "Флаг_помесячно roc_auc 0.9335714285714286\n",
      "Evaluate loss 0.7303534746170044\n",
      "================================================== EPOCH 14 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [29:49<00:00, 55.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192   3   6]\n",
      " [ 13 182   0]\n",
      " [  8   2 103]]\n",
      "Train loss 0.19803703459911048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:11<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36 14  1]\n",
      " [ 4 45  0]\n",
      " [ 2  2 24]]\n",
      "Флаг_постоплата roc_auc 0.8642729819200408\n",
      "Флаг_предоплата roc_auc 0.917592353397055\n",
      "Флаг_помесячно roc_auc 0.9392857142857143\n",
      "Evaluate loss 0.7420996427536011\n",
      "================================================== EPOCH 15 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [27:57<00:00, 52.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189   8   4]\n",
      " [  5 190   0]\n",
      " [  3   3 107]]\n",
      "Train loss 0.17365999147295952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:20<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 12  1]\n",
      " [ 5 44  0]\n",
      " [ 2  2 24]]\n",
      "Флаг_постоплата roc_auc 0.8884644766997708\n",
      "Флаг_предоплата roc_auc 0.9289589253422889\n",
      "Флаг_помесячно roc_auc 0.9392857142857144\n",
      "Evaluate loss 0.6930805444717407\n",
      "================================================== EPOCH 16 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [28:46<00:00, 53.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[192   3   6]\n",
      " [  6 189   0]\n",
      " [  4   0 109]]\n",
      "Train loss 0.15896614908706397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:10<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  5  4]\n",
      " [ 8 40  1]\n",
      " [ 2  2 24]]\n",
      "Флаг_постоплата roc_auc 0.9057804940157881\n",
      "Флаг_предоплата roc_auc 0.9325755618703179\n",
      "Флаг_помесячно roc_auc 0.9439285714285715\n",
      "Evaluate loss 0.5895752310752869\n",
      "================================================== EPOCH 17 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [28:43<00:00, 53.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194   1   6]\n",
      " [  2 193   0]\n",
      " [  3   0 110]]\n",
      "Train loss 0.11577735334867612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:10<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 10  3]\n",
      " [ 3 46  0]\n",
      " [ 3  2 23]]\n",
      "Флаг_постоплата roc_auc 0.8854087089381207\n",
      "Флаг_предоплата roc_auc 0.9256006199948333\n",
      "Флаг_помесячно roc_auc 0.9407142857142858\n",
      "Evaluate loss 0.7650834918022156\n",
      "================================================== EPOCH 18 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [28:58<00:00, 54.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196   2   3]\n",
      " [  2 193   0]\n",
      " [  3   2 108]]\n",
      "Train loss 0.09323560539633036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [01:08<00:00,  8.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  8  3]\n",
      " [ 1 48  0]\n",
      " [ 2  2 24]]\n",
      "Флаг_постоплата roc_auc 0.9022154316271963\n",
      "Флаг_предоплата roc_auc 0.9679669336088865\n",
      "Флаг_помесячно roc_auc 0.9321428571428572\n",
      "Evaluate loss 0.6199560761451721\n",
      "================================================== EPOCH 19 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▌                                      | 4/32 [04:29<31:28, 67.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCH_NUM):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     evaluate(model, dev_iterator, estate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#pred = np.argmax(pred, axis = 1) \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#true = np.argmax(true, axis = 1)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(pred, true)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#correct_predictions += torch.sum(pred == y)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator, estate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29e91880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:56<00:00,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84        51\n",
      "           1       0.82      0.96      0.89        49\n",
      "           2       0.89      0.86      0.87        28\n",
      "\n",
      "    accuracy                           0.87       128\n",
      "   macro avg       0.87      0.87      0.87       128\n",
      "weighted avg       0.87      0.87      0.87       128\n",
      "\n",
      "[[40  8  3]\n",
      " [ 2 47  0]\n",
      " [ 2  2 24]]\n",
      "Флаг_постоплата roc_auc 0.9017061370002547\n",
      "Флаг_предоплата roc_auc 0.968741927150607\n",
      "Флаг_помесячно roc_auc 0.9307142857142857\n",
      "Evaluate loss 0.6531900763511658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'saved_weights_tables_16.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "evaluate(model, dev_iterator, estate  = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084c4e7",
   "metadata": {},
   "source": [
    "### создаем лист из предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1901a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = []\n",
    "for i in data_clean.index:\n",
    "    data_array.extend([data_clean[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c68aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_sber(data_array, padding=True, truncation=True, max_length=48, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a266808",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokens.input_ids.numpy() # эмбед-токена\n",
    "mask = tokens.attention_mask.numpy() # токен-маска\n",
    "token_type = tokens.token_type_ids.numpy() # тип токена"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5245bac",
   "metadata": {},
   "source": [
    "### скачиваем данные для будущей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save('bert_process/input_ids.npy', input_ids)\n",
    "save('bert_process/mask.npy', mask)\n",
    "save('bert_process/token_type.npy', token_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcde61",
   "metadata": {},
   "source": [
    "### применяем модель - получаем тензор (эмбеддинг) для токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model_sber(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49798ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4e808",
   "metadata": {},
   "source": [
    "### сжимаем эмбеддинг по столбцу - получаем эмбеддинг для вектора сглаживанием средних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tokens['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()\n",
    "masked_embeddings = torch.sum(embeddings * mask, 1)\n",
    "sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "sent_embeddings = masked_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save('bert_process/bert_output.npy', sent_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d918db",
   "metadata": {},
   "source": [
    "### Аналогично выполняем для преобразования тестовой части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c48d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_test = []\n",
    "for i in range(len(data_clean_test)):\n",
    "    data_array_test.extend([data_clean_test['Шаблон'].iloc[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea12f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = tokenizer_sber(data_array_test, padding=True, truncation=True, max_length=48, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = tokens_test.input_ids.numpy()\n",
    "mask_test = tokens_test.attention_mask.numpy()\n",
    "token_type_test = tokens_test.token_type_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save('bert_process/input_ids_test.npy', input_ids_test)\n",
    "save('bert_process/mask_test.npy', mask_test)\n",
    "save('bert_process/token_type_test.npy', token_type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output_test = model_sber(**tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = model_output_test.last_hidden_state\n",
    "mask = tokens_test['attention_mask'].unsqueeze(-1).expand(embeddings_test.size()).float()\n",
    "masked_embeddings = torch.sum(embeddings_test * mask, 1)\n",
    "sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "sent_embeddings_test = masked_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save('bert_process/bert_output_test.npy', sent_embeddings_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa53528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
